# ADS-B Historical Data Collection

Download, process, and analyze historical ADS-B data from [adsb.lol/globe_history_2025](https://github.com/adsblol/globe_history_2025).

## ðŸ“‹ Prerequisites

- Node.js 18+
- AWS Account with S3 access
- ~20GB disk space for temporary downloads (per week of data)

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
cd adsb-history
npm install
```

### 2. Configure AWS Credentials

Create `.env` file from the example:

```bash
cp .env.example .env
```

Edit `.env` and add your AWS credentials:

```env
AWS_REGION=us-west-2
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
S3_BUCKET_NAME=ayryx-adsb-history
```

### 3. Download Recent Week of Data

**Option A: Automated EC2 (Recommended)**

Launch an EC2 instance that automatically downloads and uploads to S3:

```bash
./scripts/provision-ec2-downloader.sh --start-date 2025-11-02 --days 7
```

The EC2 instance will self-terminate when complete. See [EC2_INGESTION_README.md](./EC2_INGESTION_README.md) for details.

**Benefits:**
- No local disk space required
- Fast network transfer (EC2 â†’ S3)
- Auto-terminates when complete
- Cost: < $0.10 per run

**Option B: Local Machine**

If you have 30GB+ free disk space:

```bash
npm run download-week
```

This will:
1. Download split tar files (`.tar.aa` + `.tar.ab`) from GitHub releases (~3GB)
2. Concatenate into single tar file
3. Upload to S3 at `raw/YYYY/MM/DD/` (tar only, not extracted)
4. Extract temporarily to verify structure
5. Clean up all temporary files

**Note:** We only store the compressed tar files in S3 (~3GB per day). Extracted data (~20GB per day) is generated on-demand during processing to save storage costs.

### 4. Custom Date Ranges

Download specific dates:

```bash
# Using EC2 (recommended)
./scripts/provision-ec2-downloader.sh --start-date 2025-11-02 --days 7

# Using local machine (requires 30GB+ disk space)
node scripts/download-week.js --start-date 2025-11-02 --days 7

# Download just 3 days
./scripts/provision-ec2-downloader.sh --start-date 2025-11-06 --days 3
```

## ðŸ“‚ Directory Structure

```
adsb-history/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ airports.json          # Airports to analyze
â”‚   â”œâ”€â”€ aws-config.json         # S3/CloudFront configuration
â”‚   â””â”€â”€ processing-config.json  # Processing parameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ GitHubReleaseDownloader.js
â”‚   â”‚   â”œâ”€â”€ DataExtractor.js
â”‚   â”‚   â””â”€â”€ S3Uploader.js
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.js
â”‚       â””â”€â”€ s3.js
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download-week.js        # Download recent week
â”œâ”€â”€ temp/                       # Temporary downloads (auto-cleaned)
â””â”€â”€ logs/                       # Application logs
```

## â˜ï¸ S3 Storage Structure

Data is organized in S3 as:

```
s3://ayryx-adsb-history/
â”œâ”€â”€ raw/                          # Tar archives only (~3GB/day, ~1TB/year)
â”‚   â””â”€â”€ 2025/
â”‚       â””â”€â”€ 11/
â”‚           â””â”€â”€ 08/
â”‚               â””â”€â”€ v2025.11.08-planes-readsb-prod-0.tar
â”‚
â””â”€â”€ api/                          # Pre-computed stats (coming soon)
    â””â”€â”€ KLAX/
        â””â”€â”€ approaches/
            â””â”€â”€ all-time.json
```

**Tar contents** (extracted on-demand during processing):
- `./traces/d0/`, `./traces/d1/`, ... `./traces/ff/` - Flight traces by ICAO hex
- `./acas/` - Collision avoidance data

## ðŸ”§ Available Scripts

- `npm run download-week` - Download recent 7 days
- `npm run daily-update` - (Coming soon) Daily incremental update
- `npm run backfill` - (Coming soon) Bulk historical download

## ðŸ“Š Data Source

**adsb.lol globe_history_2025**: https://github.com/adsblol/globe_history_2025/releases

- Daily releases with format: `v2025.11.08-planes-readsb-prod-0`
- Split tar archives: `.tar.aa` (~2GB) + `.tar.ab` (~1GB)
- Contains global ADS-B position reports in readsb JSON format

## ðŸ› Troubleshooting

### AWS Credentials Not Found

Make sure you've created `.env` file with valid AWS credentials:

```bash
cp .env.example .env
# Edit .env with your credentials
```

### Out of Disk Space

The download script cleans up temporary files automatically. If you run out of space:

```bash
rm -rf temp/
```

### GitHub Rate Limits

GitHub allows 60 API requests per hour for unauthenticated requests. This should be sufficient for downloading a few days of data at a time. If you need to download more frequently, space out your downloads.

## ðŸ“– Next Steps

After downloading raw data:

1. **Processing**: Implement flight track building and metrics calculation
2. **Analysis**: Extract airport-specific statistics
3. **API Generation**: Create pre-computed JSON files for frontend
4. **CloudFront**: Deploy CDN for fast global access

See [ARCHITECTURE.md](./ARCHITECTURE.md) for complete system design.

## ðŸ“ License

Data from adsb.lol is provided under Open Database License (ODbL) + CC0.
